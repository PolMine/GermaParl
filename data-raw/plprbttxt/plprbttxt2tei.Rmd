---
title: "Generate TEI from plain text protocols"
author: "Andreas Blaette"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Generate TEI from plain text protocols}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

## Getting started

```{r load_frap}
required_package_version <- "0.4.1.9005"
if (packageVersion("frappp") < as.package_version(required_package_version))
  stop("version requirement not met")
library(frappp)
packageVersion("frappp")
```

```{r load_frap}
library(R6)
library(xml2)
library(magrittr)
library(pbapply)
library(ctk)
```


## Getting input files

```{r, eval = TRUE}
txt_repo_dir_utf8 <- "~/Lab/gitlab/plprbttxt_txt_utf8"
txt_repo_dir_raw <- "~/Lab/gitlab/plprbttxt_txt_raw"
download_report_file <- file.path(txt_repo_dir_raw, "downloadReport.csv")
lp <- "18"

# tei_outdir <- "~/Lab/gitlab/plprbttxt_tei"
tei_outdir <- "~/Lab/tmp/plprbttxt_tei"

files <- Sys.glob(paths = sprintf("%s/BT_*.txt", txt_repo_dir_utf8))
files <- sample(x = files, size = 10)
```


## Define parser

```{r}
GermaParlParserTXT <- R6Class(
  
  "GermaParlParserTXT",
  inherit = PlenaryProtocolParser,
  
  public = list(
    
    xmldoc = NULL, # class 'xml_document' from packabe 'xml2'
    download_report = NULL, # a data.frame
    
    initialize = function(download_report_file){
      
      self$download_report <- read.table(file = download_report_file, stringsAsFactors = FALSE)
      self$xpath <- teiXpath
      
      invisible(self)
    },
    
    read_file = function(filename){
      self$id <- gsub("(BT_\\d+_\\d+)\\.txt", "\\1", basename(filename))
      self$txt <- readLines(filename)
      invisible(self)
    },
    
    get_metadata = function(){
      
      self$metadata = list(
        title = "Plenarprotokoll",
        publisher = "Deutscher Bundestag",
        package = "frappp",
        version = as.character(packageVersion("frappp")),
        filetype = "txt",
        birthday = Sys.Date()
      )
      
      self$metadata <- c(
        self$metadata,
        list(
          sessionNo = gsub("^BT_\\d+_(\\d+).*$", "\\1", self$id),
          legislativePeriod = gsub("^BT_(\\d+)_.*$", "\\1", self$id),
          date = ctk::normalizeGermanDate(
            gsub(
              "^.*?,\\s+.*?(\\s+den|,\\s+den|,)\\s+(.*)$", "\\2",
              grep("^.*\\s+\\d{4}$", self$txt[1L:20L], value = TRUE)[1]
            )
          )
        )
      )

      download_report_subset <- subset(
        self$download_report,
        self$download_report[,1] == paste(self$id, "txt", sep = ".")
      )
      self$metadata[["url"]] <- download_report_subset[,2]
      self$metadata[["retrievalDate"]] <- download_report_subset[,3]
      
      invisible(self)
    },
    
    preprocessing = function(){
		  txt <- self$txt
		  Encoding(txt) <- "UTF-8"
		  
		  # remove empty lines
		  emptyLines <- grep("^\\s*$", txt)
		  if (length(emptyLines) > 0) txt <- txt[-emptyLines]
		  
		  txt <- gsub("[\025]", "\u00A7", txt)
		  txt <- gsub("\u00A0", " ", txt) # removes NO-BREAK SPACE, e.g. in B\u00DCNDNIS 90/DIE GR\u00DCNEN
		  txt <- gsub("\a", "\n", txt)
		  txt <- gsub("^\\s*(.*)\\s*$", "\\1", txt) # remove leading and trailing whitespace
		  txtOne <- paste(txt, collapse = "BREAK")
		  txtOne <- gsub("CDU/BREAKCSU", "CDU/CSU", txtOne)
		  txtOne <- gsub("B\u00DCNDNIS\\s+90/BREAKD[iI][eE]\\s+GR\u00DCNEN", "B\u00DCNDNIS 90/DIE GR\u00DCNEN", txtOne)
		  txtOne <- gsub("B\u00DCNDNIS\\s+90/DIE\\s+GR\u00DC-BREAKNEN", "B\u00DCNDNIS 90/DIE GR\u00DCNEN", txtOne)
		  txt <- strsplit(txtOne, "BREAK")[[1]]
		  txt <- gsub("Nr\\.(\\d)", "Nr. \\1", txt)
		  txt <- gsub("Abs\\.(\\d)", "Abs. \\1", txt)
		  txt <- gsub("([a-z])\\.\\d+", "\\1.", txt)
		  txt <- gsub("\\s+", " ", txt)
		  
		  if (self$id %in% names(self$preprocessing_functions)){
		    txt <- self$preprocessing_functions[[self$id]](txt)
		  }
		  self$txt <- txt
		},
		
		get_end_of_debate = function(){
		  
		  end_of_debate <- grep("^\\s*\\(\\s*Schlu(ss|.)( der Sitzung|).*?Uhr", self$txt, perl = TRUE)[1]
		  if (length(end_of_debate) == 0 || is.na(end_of_debate)) {
		    end_of_debate <- grep("Die\\sSitzung\\sist\\sgeschlossen", self$txt, perl = TRUE) + 1L
		  }
		  if (length(end_of_debate) == 0 || is.na(end_of_debate)) {
		    end_of_debate <- length(self$txt)
		    message("end of debate not found in ", self$id)
		  }
		  self$txt <- self$txt[1L:(end_of_debate[1] - 1L)]
		  invisible(self)
		},
		
		remove_constituency_from_name = function(){
		  self$chunk_data[["who"]] <- gsub("\\s+\\(.*?\\)\\s*$", "", self$chunk_data[["who"]])
		  invisible(self)
		},

    reconstruct_paragraphs = function(exclude = "pres", regex = "[\\.\\):!?]\\s*$"){
      chunks_rework <- lapply(
        1L:length(self$chunks),
        function(chunk_no){
          chunk <- self$chunks[[chunk_no]]
          if (names(self$chunks)[chunk_no] %in% exclude){
            return( chunk )
          } else {
            to_concatenate <- !grepl(regex, chunk)
            if (any(to_concatenate)){
              for (i in rev(which(to_concatenate == TRUE))){
                # do not concatenate open-ended line and the next paragraph,
                # if the latter is an interjection
                if (grepl("^\\s*\\(.*\\)\\s*$", chunk[i + 1L]) == FALSE){
                  chunk[i] <- paste(chunk[i], chunk[i + 1L], sep = " ")
                  chunk <- chunk[-(i + 1L)]              
                }
              }
            }
            return(chunk)
          }
        })
      names(chunks_rework) <- names(self$chunks)
      self$chunks <- chunks_rework
      invisible(self)
    }

  )
)
```





## Configure the parser

We need to construct the regex for parliamentarians in advance ... 

```{r}
auxfiles_dir <- "~/Lab/github/GermaParl/data-raw/auxfiles"
```

```{r}
agenda_item_regex <- eval(parse(file.path(auxfiles_dir, "agenda_item_regex.R")))
agenda_item_regex_supplement <- eval(parse(file.path(auxfiles_dir, "agenda_item_regex_supplement.R")))
```


```{r}
mp_regex <- eval(parse(file.path(auxfiles_dir, "parliamentary_groups_regex.R"))) %>%
  unname() %>%
  sprintf("\\s*%s\\s*", .) %>%
  paste(collapse = "|") %>%
  sprintf("^\\s*(?!\\()(?!(Vizepräsident|Präsident))(?!.*?Parl\\.\\s+Staatssekretär)(.+?)\\s*\\((%s)\\)(|\\s*\\(von\\s.*?mit Beifall be\\s?grüßt\\))\\s*:\\s*(.*?)$", .)
```


```{r}
GPP <- GermaParlParserTXT$new(download_report_file = download_report_file)
```


```{r}
GPP$preprocessing_functions <- eval(parse(file.path(auxfiles_dir, "preprocessing_functions.R")))

GPP$speaker_regex <- eval(parse(file.path(auxfiles_dir, "speaker_regex.R")))
GPP$speaker_mismatch <- eval(parse(file.path(auxfiles_dir, "speaker_mismatch.R")))
GPP$speaker_maximum_nchar_name <- 50L

GPP$agenda_item_regex <- eval(parse(file.path(auxfiles_dir, "agenda_item_regex.R")))
GPP$agenda_item_types <- eval(parse(file.path(auxfiles_dir, "agenda_item_types.R")))
GPP$agenda_item_mismatch <- eval(parse(file.path(auxfiles_dir, "agenda_item_mismatch.R")))
GPP$agenda_item_maximum_nchar_desc <- 80L

GPP$stage_regex <- c(interjection = "^\\s*\\(.*?\\)\\s*$")
GPP$stage_match_n_lines <- 3L
GPP$stage_mismatch_regex <- "Beratung\\s+\\d+\\.\\s+Sitzung"
```





```{r processing_multiple_docs, message = FALSE}
y <- lapply(
  files,
  function(file){
    print(file)
    GPP$read_file(filename = file)
    GPP$agenda_item_regex <- c(
      agenda_item_regex,
      agenda_item_regex_supplement[[GPP$id]]
    )
    GPP$preprocessing()
    GPP$get_end_of_debate()
    GPP$get_metadata()
    GPP$make_header()
    GPP$split_by_speakers()
    GPP$detect_stage_regex_matches()
    GPP$remove_constituency_from_name()
    GPP$reconstruct_paragraphs(exclude = "pres")
    GPP$make_body()
    writeLines(
      text = as.character(GPP$xml),
      con = file.path(tei_outdir, paste(GPP$id, "xml", sep = "."))
    )
    invisible( NULL )
  }
)
```



## Enriching the protocols

The following steps may have to be repeated until the data are clean:

- match a key generated from the speaker attributes against the database
- add the information to the TEI documents
- inspect whether there is still information missing
- pimp the alias file, repair wikipedia data etc


```{r initialize_enhancer, eval = TRUE}
datafiles <- c(
  government = system.file("csv", "de_gov.csv", package = "actors"),
  mp = system.file("csv", "bt_mp.csv", package = "actors"),
  presidency = system.file("csv", "bt_pres.csv", package = "actors"),
  federal_council = system.file("csv", "bt_federal_council.csv", package = "actors"),
  parliamentary_commissioner = system.file("csv", "bt_parliamentary_commissioner.csv", package = "actors"),
  supplement = system.file("csv", "de_supplement.csv", package = "actors")
)
```

```{r eval = TRUE}
sourceDir <- tei_outdir
targetDir <- "~/Lab/tmp/plprbttxt_tei2"

E <- Enhancer$new(sourceDir = sourceDir, targetDir = targetDir, datafiles = datafiles)
E$aliases <- eval(parse(file.path(auxfiles_dir, "speaker_alias.R")))
E$getSpeakerAttributes()
E$consolidateAndEnrichSpeakerattributes()
pblapply(list.files(sourceDir), E$enhanceFile)
```


## Consolidation

```{r enhance, eval = TRUE}
E2 <- Enhancer$new(sourceDir = targetDir)
E2$getSpeakerAttributes(element = "sp", attrs = c("who", "role", "name", "parliamentary_group", "party"))
dfs <- E2$analyseMissingInformation()
```


## Create html for error checking

```{r html, eval = FALSE}
teiToHtml(
  BT, sourceDir = "tei_enriched", targetDir = "html",
  progress = TRUE, mc = FALSE
  )
```


